# Complete Setup Guide - Chrome Extension + API + SQLite

This guide shows you how to set up the complete system with the Chrome extension saving data to a Node.js API with a real SQLite database.

---

## ğŸ¯ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Chrome Extension                         â”‚
â”‚  (Scrapes Collabstr profiles from browser)                 â”‚
â”‚                                                             â”‚
â”‚  - content.js: Extracts profile IDs                        â”‚
â”‚  - background.js: Sends data to API                        â”‚
â”‚  - popup.js: UI controls                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP POST/GET
                       â”‚ http://localhost:4000/api
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Node.js API Server                       â”‚
â”‚  (Express server handling data)                            â”‚
â”‚                                                             â”‚
â”‚  - Receives scraped profiles                               â”‚
â”‚  - Validates and processes data                            â”‚
â”‚  - Manages scraping sessions                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ SQL queries
                       â”‚ better-sqlite3
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SQLite Database                          â”‚
â”‚  (Real database file: collabstr_profiles.db)               â”‚
â”‚                                                             â”‚
â”‚  - profiles table: id, page, scraped_at, session_id        â”‚
â”‚  - sessions table: metadata about scraping runs            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Step 1: Install API Server

### Navigate to API folder:
```bash
cd api
```

### Install dependencies:
```bash
npm install
```

This installs:
- **express**: Web server framework
- **cors**: Allow Chrome extension requests
- **better-sqlite3**: Fast native SQLite
- **body-parser**: Parse JSON requests

---

## ğŸš€ Step 2: Start API Server

### Option A: Production Mode
```bash
npm start
```

### Option B: Development Mode (auto-reload)
```bash
npm run dev
```

You should see:
```
=================================
  Collabstr Scraper API Server
=================================
  ğŸš€ Server running on port 4000
  ğŸ“Š Database: /path/to/collabstr_profiles.db
  ğŸŒ API: http://localhost:4000
=================================
```

**Keep this terminal open!** The server must be running for the extension to save data.

---

## ğŸ”§ Step 3: Setup Chrome Extension

### Generate Icons (if not done):
Open `generate-icons.html` in browser â†’ Download all 3 icons

### Load Extension in Chrome:
1. Go to `chrome://extensions/`
2. Enable **Developer mode** (top-right toggle)
3. Click **Load unpacked**
4. Select the `Pluign` folder (parent folder, not api folder)
5. Extension loads âœ“

**No more npm install needed for extension!** We removed sql.js completely.

---

## âœ… Step 4: Verify API Connection

### Check Extension Background Console:
1. Go to `chrome://extensions/`
2. Find "Collabstr Profile Scraper"
3. Click **"service worker"**
4. Look for this message:
   ```
   [Background] âœ“ API server connected: Collabstr Scraper API is running
   ```

If you see this, everything is connected! âœ…

If you see:
```
[Background] âš  API server not reachable
```
Make sure the API server is running (Step 2).

---

## ğŸ¬ Step 5: Start Scraping

### 1. Navigate to Collabstr
```
https://collabstr.com/influencers?ph_id=622942&p=instagram
```

### 2. Apply Your Filters
- Platform: Instagram, TikTok, YouTube, etc.
- Niche: Beauty, Tech, Fashion, etc.
- Followers: Set min/max
- Location: Any country

### 3. Click Extension Icon
Click the purple extension icon in your toolbar

### 4. Click "Start Scraping"
Watch it work:
- Page scrolls automatically âœ“
- Profiles extracted âœ“
- Data sent to API âœ“
- Saved to SQLite database âœ“
- Moves to next page âœ“

---

## ğŸ“Š Step 6: View Your Data

### Method 1: API Endpoints

**Get Statistics:**
```bash
curl http://localhost:4000/api/stats
```

Response:
```json
{
  "totalProfiles": 240,
  "uniqueProfiles": 235,
  "totalPages": 10,
  "profilesPerPage": [...]
}
```

**Export CSV:**
```
http://localhost:4000/api/export/csv
```
Opens download dialog with CSV file

### Method 2: Direct Database Access

**Using SQLite Command Line:**
```bash
cd api
sqlite3 collabstr_profiles.db
```

```sql
SELECT * FROM profiles LIMIT 10;
SELECT COUNT(*) FROM profiles;
SELECT DISTINCT id FROM profiles;
```

**Using DB Browser for SQLite:**
1. Download [DB Browser for SQLite](https://sqlitebrowser.org/)
2. Open `api/collabstr_profiles.db`
3. Browse data visually
4. Export to CSV/JSON/Excel

### Method 3: Export Button in Extension

Click **"Export CSV"** button in extension popup â†’ Downloads CSV with all profiles

---

## ğŸ—„ï¸ Database Structure

### profiles table
```sql
CREATE TABLE profiles (
  id TEXT,              -- Profile username
  page INTEGER,         -- Page number (1, 2, 3...)
  scraped_at DATETIME,  -- Timestamp
  session_id TEXT,      -- Scraping session ID
  PRIMARY KEY (id, page, session_id)
)
```

### sessions table
```sql
CREATE TABLE sessions (
  session_id TEXT PRIMARY KEY,
  started_at DATETIME,
  ended_at DATETIME,
  total_profiles INTEGER,
  total_pages INTEGER,
  filters TEXT          -- JSON of applied filters
)
```

---

## ğŸ”„ Workflow

### Complete Scraping Session:

1. **Start API** â†’ `npm start` in api folder
2. **Load Extension** â†’ Chrome loads background.js
3. **Background connects to API** â†’ Verifies server is running
4. **Navigate to Collabstr** â†’ Apply filters
5. **Click "Start Scraping"** â†’ Extension sends START_SCRAPING
6. **Background starts session** â†’ POST /api/session/start
7. **Content script scrapes page** â†’ Extracts profile IDs
8. **Background receives IDs** â†’ Formats data
9. **Background sends to API** â†’ POST /api/profiles
10. **API saves to SQLite** â†’ INSERT INTO profiles
11. **Extension moves to next page** â†’ Repeat steps 7-10
12. **No more pages** â†’ Extension sends STOP_SCRAPING
13. **Background ends session** â†’ POST /api/session/end
14. **Done!** â†’ Data in SQLite database

---

## ğŸ› ï¸ Configuration

### Change API Port

Edit `api/server.js`:
```javascript
const PORT = process.env.PORT || 4000; // Change to 5000, 3000, etc.
```

Also update `background.js`:
```javascript
const API_URL = 'http://localhost:4000/api'; // Match new port
```

### API on Different Machine

If API runs on another computer:

1. Edit `api/server.js`:
```javascript
app.listen(PORT, '0.0.0.0', () => {
  // Binds to all network interfaces
});
```

2. Find API server's IP address
3. Update `background.js`:
```javascript
const API_URL = 'http://192.168.1.XXX:4000/api';
```

---

## ğŸ› Troubleshooting

### Extension Can't Connect to API

**Problem:** `âš  API server not reachable`

**Solutions:**
1. Check API server is running: `npm start` in api folder
2. Check port 4000 is not blocked by firewall
3. Test manually: Visit `http://localhost:4000/` in browser
4. Check background.js has correct URL

### Database Locked

**Problem:** `Error: database is locked`

**Solutions:**
1. Close DB Browser for SQLite
2. Close any other programs accessing the .db file
3. Restart API server

### Port Already in Use

**Problem:** `EADDRINUSE: address already in use :::4000`

**Solutions:**
1. Kill process on port 4000:
   ```bash
   # Windows
   netstat -ano | findstr :4000
   taskkill /PID <PID> /F

   # Mac/Linux
   lsof -i :4000
   kill -9 <PID>
   ```
2. Or change port in server.js

### No Profiles Being Saved

**Check these:**
1. âœ“ API server running
2. âœ“ Extension connected to API
3. âœ“ Content script finding profiles (check page console)
4. âœ“ No errors in API server terminal
5. âœ“ Check `api/collabstr_profiles.db` file exists

---

## ğŸ“ Project Structure

```
Pluign/
â”œâ”€â”€ manifest.json          # Extension config
â”œâ”€â”€ background.js          # Sends data to API â† NO MORE SQL.JS!
â”œâ”€â”€ content.js             # Scrapes pages
â”œâ”€â”€ popup.html             # UI
â”œâ”€â”€ popup.js               # UI logic
â”œâ”€â”€ icon*.png              # Extension icons
â”‚
â”œâ”€â”€ api/                   # â† NEW! API Server folder
â”‚   â”œâ”€â”€ package.json       # API dependencies
â”‚   â”œâ”€â”€ server.js          # Express API with SQLite
â”‚   â”œâ”€â”€ collabstr_profiles.db  # SQLite database (created automatically)
â”‚   â””â”€â”€ README.md          # API documentation
â”‚
â””â”€â”€ SETUP_API.md           # This file
```

---

## âœ¨ Benefits of This Setup

### Before (sql.js in extension):
- âŒ WASM CSP errors
- âŒ Limited to Chrome storage (10MB)
- âŒ Difficult to query data
- âŒ No SQL tools work directly
- âŒ Complex export process

### After (API + SQLite):
- âœ… No CSP issues
- âœ… Unlimited storage
- âœ… Real SQL database
- âœ… Use any SQLite tool
- âœ… Easy CSV export
- âœ… Track multiple sessions
- âœ… Can add authentication later
- âœ… Can deploy API to cloud
- âœ… Multiple extensions can share data

---

## ğŸš€ Next Steps

### You're All Set! Now you can:

1. **Scrape profiles** â†’ Extension + API working together
2. **View in database** â†’ Use DB Browser for SQLite
3. **Export data** â†’ CSV, JSON, SQL
4. **Run queries** â†’ Full SQL power
5. **Track sessions** â†’ See all scraping runs
6. **Scale up** â†’ Deploy API to cloud later

### Advanced Usage:

- **Multiple sessions**: Run different filter combinations
- **API analytics**: Build dashboards with the data
- **Automation**: Schedule scraping with cron jobs
- **Export formats**: JSON, XML, custom formats
- **Data processing**: Python scripts to analyze profiles

---

## ğŸ“š Documentation

- **[api/README.md](api/README.md)** - Complete API documentation
- **[README.md](README.md)** - Extension documentation
- **[DEBUG.md](DEBUG.md)** - Troubleshooting guide

---

## ğŸ‰ Summary

You now have a professional-grade scraping system:

- âœ… Chrome extension scrapes data
- âœ… Node.js API processes requests
- âœ… SQLite database stores everything
- âœ… No more WASM/CSP issues
- âœ… Real database you can query
- âœ… Easy to export and analyze

**Happy scraping!** ğŸš€
